{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型的机器阅读理解\n",
    "\n",
    "阅读理解是检索问答系统中的重要组成部分，最常见的数据集是单篇章、**抽取式**阅读理解数据集。  \n",
    "\n",
    "该示例展示了如何使用PaddleNLP快速实现基于预训练模型的机器阅读理解任务。\n",
    "\n",
    "本示例使用的数据集是Dureader<sub>robust</sub>数据集。对于一个给定的问题q和一个篇章p，根据篇章内容，给出该问题的答案a。数据集中的每个样本，是一个三元组<q, p, a>，例如：\n",
    "\n",
    "**问题 q**: 乔丹打了多少个赛季\n",
    "\n",
    "**篇章 p**: 迈克尔.乔丹在NBA打了15个赛季。他在84年进入nba，期间在1993年10月6日第一次退役改打棒球，95年3月18日重新回归，在99年1月13日第二次退役，后于2001年10月31日复出，在03年最终退役…\n",
    "\n",
    "**参考答案 a**: [‘15个’,‘15个赛季’]\n",
    "\n",
    "阅读理解模型的鲁棒性是衡量该技术能否在实际应用中大规模落地的重要指标之一。随着当前技术的进步，模型虽然能够在一些阅读理解测试集上取得较好的性能，但在实际应用中，这些模型所表现出的鲁棒性仍然难以令人满意。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/67bb14fa9db64908ba03288ea1bcd49fb523f77f8adf43129293411b89a2f2a4)\n",
    "\n",
    "\n",
    "\n",
    "本示例使用的Dureader<sub>robust</sub>数据集作为首个关注阅读理解模型鲁棒性的中文数据集，旨在考察模型在真实应用场景中的过敏感性、过稳定性以及泛化能力等问题。\n",
    "\n",
    "关于该数据集的详细内容，可参考数据集[论文](https://arxiv.org/abs/2004.11142)，或官方[比赛链接](https://aistudio.baidu.com/aistudio/competition/detail/49?castk=LTE=)。\n",
    "\n",
    "## 安装说明\n",
    "\n",
    "* PaddlePaddle 安装\n",
    "\n",
    "   本项目依赖于 PaddlePaddle 2.3 及以上版本，请参考 [安装指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html) 进行安装\n",
    "\n",
    "* PaddleNLP 安装\n",
    "\n",
    "   ```shell\n",
    "   pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "   ```\n",
    "\n",
    "* 环境依赖\n",
    "\n",
    "    Python的版本要求 3.7+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台默认安装了Paddle和PaddleNLP，并定期更新版本。\n",
    "如需手动更新Paddle，可参考[飞桨安装说明](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/linux-pip.html)，安装相应环境下最新版飞桨框架。\n",
    "\n",
    "使用如下命令确保安装最新版PaddleNLP："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -m pip install paddlepaddle-gpu==2.3.0.post101 -f https://www.paddlepaddle.org.cn/whl/linux/mkl/avx/stable.html\n",
    "!pip install --upgrade paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 示例流程\n",
    "\n",
    "与大多数NLP任务相同，本次机器阅读理解任务的示例展示分为以下四步：\n",
    "\n",
    "首先我们从数据准备开始。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dd30e17318fb48fabb5701fd8a97be8176a1e372dd134cc0826e58cb5401933d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据准备\n",
    "\n",
    "数据准备流程如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/127ff46bf5c342889991438a95aa7158ec06c8852e58410ba7a516db330175f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 加载PaddleNLP内置数据集\n",
    "\n",
    "使用PaddleNLP提供的`load_dataset`API，即可一键完成数据集加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset dureader_robust (/home/aistudio/.cache/huggingface/datasets/PaddlePaddle___dureader_robust/plain_text/1.0.0/1cd8a5be26918caf884ea444d2d909d813355d7972530ea00f82fad0962f8e95)\n",
      "Reusing dataset dureader_robust (/home/aistudio/.cache/huggingface/datasets/PaddlePaddle___dureader_robust/plain_text/1.0.0/1cd8a5be26918caf884ea444d2d909d813355d7972530ea00f82fad0962f8e95)\n",
      "Reusing dataset dureader_robust (/home/aistudio/.cache/huggingface/datasets/PaddlePaddle___dureader_robust/plain_text/1.0.0/1cd8a5be26918caf884ea444d2d909d813355d7972530ea00f82fad0962f8e95)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "仙剑奇侠传3第几集上天界\n",
      "第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。\n",
      "{'text': ['第35集'], 'answer_start': [0]}\n",
      "\n",
      "燃气热水器哪个牌子好\n",
      "选择燃气热水器时，一定要关注这几个问题：1、出水稳定性要好，不能出现忽热忽冷的现象2、快速到达设定的需求水温3、操作要智能、方便4、安全性要好，要装有安全报警装置 市场上燃气热水器品牌众多，购买时还需多加对比和仔细鉴别。方太今年主打的磁化恒温热水器在使用体验方面做了全面升级：9秒速热，可快速进入洗浴模式；水温持久稳定，不会出现忽热忽冷的现象，并通过水量伺服技术将出水温度精确控制在±0.5℃，可满足家里宝贝敏感肌肤洗护需求；配备CO和CH4双气体报警装置更安全（市场上一般多为CO单气体报警）。另外，这款热水器还有智能WIFI互联功能，只需下载个手机APP即可用手机远程操作热水器，实现精准调节水温，满足家人多样化的洗浴需求。当然方太的磁化恒温系列主要的是增加磁化功能，可以有效吸附水中的铁锈、铁屑等微小杂质，防止细菌滋生，使沐浴水质更洁净，长期使用磁化水沐浴更利于身体健康。\n",
      "{'text': ['方太'], 'answer_start': [110]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\r\n",
    "\r\n",
    "train_examples = load_dataset('PaddlePaddle/dureader_robust', split=\"train\")\r\n",
    "dev_examples = load_dataset('PaddlePaddle/dureader_robust', split=\"validation\")\r\n",
    "test_examples = load_dataset('PaddlePaddle/dureader_robust', split=\"test\")\r\n",
    "\r\n",
    "\r\n",
    "for idx in range(2):\r\n",
    "    print(train_examples[idx]['question'])\r\n",
    "    print(train_examples[idx]['context'])\r\n",
    "    print(train_examples[idx]['answers'])\r\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "关于更多PaddleNLP数据集，请参考[数据集列表](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_list.html)\n",
    "\n",
    "如果你想使用自己的数据集文件构建数据集，请参考[以内置数据集格式读取本地数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_load.html#id4)和[自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 加载 `paddlenlp.transformers.AutoTokenizer`用于数据处理\n",
    "\n",
    "DuReader<sub>rubust</sub>数据集采用SQuAD数据格式，InputFeature使用滑动窗口的方法生成，即一个example可能对应多个InputFeature。\n",
    "\n",
    "由于文章加问题的文本长度可能大于max_seq_length，答案出现的位置有可能出现在文章最后，所以不能简单的对文章进行截断。\n",
    "\n",
    "那么对于过长的文章，则采用滑动窗口将文章分成多段，分别与问题组合。再用对应的tokenizer转化为模型可接受的feature。doc_stride参数就是每次滑动的距离。滑动窗口生成InputFeature的过程如下图：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5776cf9ec00546bca047a0930c8f56a8b64d723e0ff04f269334522954bb7d90)\n",
    "\n",
    "本基线中，我们使用的预训练模型是ERNIE，ERNIE对中文数据的处理是以字为单位。**PaddleNLP对于各种预训练模型已经内置了相应的tokenizer**，指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "tokenizer的作用是将原始输入文本转化成模型可以接受的输入数据形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\n",
    "from paddlenlp.transformers import AutoTokenizer\n",
    "\n",
    "# 设置模型名称\n",
    "MODEL_NAME = 'ernie-3.0-medium-zh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 调用`map()`方法批量处理数据\n",
    "\n",
    "由于我们传入了`lazy=False`，所以我们使用`load_dataset()`自定义的数据集是`MapDataset`对象。`MapDataset`是`paddle.io.Dataset`的功能增强版本。其内置的`map()`方法适合用来进行批量数据集处理。\n",
    "\n",
    "`map()`方法接受的主要参数是一个用于数据处理的function。正好可以与tokenizer相配合。\n",
    "\n",
    "以下是本示例中的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import prepare_train_features, prepare_validation_features\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "max_seq_length = 512\r\n",
    "doc_stride = 128\r\n",
    "\r\n",
    "train_trans_func = partial(prepare_train_features, \r\n",
    "                           max_seq_length=max_seq_length, \r\n",
    "                           doc_stride=doc_stride,\r\n",
    "                           tokenizer=tokenizer)\r\n",
    "\r\n",
    "\r\n",
    "dev_trans_func = partial(prepare_validation_features, \r\n",
    "                           max_seq_length=max_seq_length, \r\n",
    "                           doc_stride=doc_stride,\r\n",
    "                           tokenizer=tokenizer)\r\n",
    "    \r\n",
    "column_names = train_examples.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = train_examples.map(train_trans_func, batched=True, num_proc=4, remove_columns=column_names)\n",
    "dev_ds = dev_examples.map(dev_trans_func, batched=True, num_proc=4, remove_columns=column_names)\n",
    "test_ds = test_examples.map(dev_trans_func, batched=True, num_proc=4, remove_columns=column_names)\n",
    "\n",
    "dev_ds_for_model = dev_ds.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_ds_for_model = test_ds.remove_columns([\"example_id\", \"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1034, 1189, 734, 2003, 241, 284, 131, 553, 271, 28, 125, 280, 2, 131, 1773, 271, 1097, 373, 1427, 1427, 501, 88, 662, 1906, 4, 561, 125, 311, 1168, 311, 692, 46, 430, 4, 84, 2073, 14, 1264, 3967, 5, 1034, 1020, 1829, 268, 4, 373, 539, 8, 154, 5210, 4, 105, 167, 59, 69, 685, 12043, 539, 8, 883, 1020, 4, 29, 720, 95, 90, 427, 67, 262, 5, 384, 266, 14, 101, 59, 789, 416, 237, 12043, 1097, 373, 616, 37, 1519, 93, 61, 15, 4, 255, 535, 7, 1529, 619, 187, 4, 62, 154, 451, 149, 12043, 539, 8, 253, 223, 3679, 323, 523, 4, 535, 34, 87, 8, 203, 280, 1186, 340, 9, 1097, 373, 5, 262, 203, 623, 704, 12043, 84, 2073, 1137, 358, 334, 702, 5, 262, 203, 4, 334, 702, 405, 360, 653, 129, 178, 7, 568, 28, 15, 125, 280, 518, 9, 1179, 487, 12043, 84, 2073, 1621, 1829, 1034, 1020, 4, 539, 8, 448, 91, 202, 466, 70, 262, 4, 638, 125, 280, 83, 299, 12043, 539, 8, 61, 45, 7, 1537, 176, 4, 84, 2073, 288, 39, 4, 889, 280, 14, 125, 280, 156, 538, 12043, 190, 889, 280, 71, 109, 124, 93, 292, 889, 46, 1248, 4, 518, 48, 883, 125, 12043, 539, 8, 268, 889, 280, 109, 270, 4, 1586, 845, 7, 669, 199, 5, 3964, 3740, 1084, 4, 255, 440, 616, 154, 72, 71, 109, 12043, 49, 61, 283, 3591, 34, 87, 297, 41, 9, 1993, 2602, 518, 52, 706, 109, 12043, 37, 10, 561, 125, 43, 8, 445, 86, 576, 65, 1448, 2969, 4, 469, 1586, 118, 776, 5, 1993, 2602, 4, 108, 25, 179, 51, 1993, 2602, 498, 1052, 122, 12043, 1082, 1994, 1616, 11, 262, 4, 518, 171, 813, 109, 1084, 270, 12043, 539, 8, 3006, 580, 11, 31, 4, 2473, 306, 34, 87, 889, 280, 846, 573, 12043, 561, 125, 14, 539, 889, 810, 276, 182, 4, 67, 351, 14, 889, 1182, 118, 776, 156, 952, 4, 539, 889, 16, 38, 4, 445, 15, 200, 61, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "14\n",
      "16\n",
      "[1, 1404, 266, 506, 101, 361, 1256, 27, 664, 85, 170, 2, 352, 790, 1404, 266, 506, 101, 361, 36, 4, 7, 91, 41, 129, 490, 47, 553, 27, 358, 281, 74, 208, 6, 39, 101, 862, 91, 92, 41, 170, 4, 16, 52, 39, 87, 1745, 506, 1745, 888, 5, 87, 528, 249, 6, 532, 537, 45, 302, 94, 91, 5, 413, 323, 101, 565, 284, 6, 868, 25, 41, 826, 52, 6, 58, 518, 397, 6, 204, 62, 92, 41, 170, 4, 41, 371, 9, 204, 62, 337, 1023, 371, 521, 99, 191, 28, 1404, 266, 506, 101, 361, 100, 664, 539, 65, 4, 817, 1042, 36, 201, 413, 65, 120, 51, 277, 14, 2081, 541, 1190, 348, 12043, 58, 512, 508, 17, 57, 445, 5, 1512, 73, 1664, 565, 506, 101, 361, 11, 175, 29, 82, 412, 58, 76, 388, 15, 62, 76, 658, 222, 74, 701, 1866, 537, 506, 4, 48, 532, 537, 71, 109, 1123, 1600, 469, 220, 12048, 101, 565, 303, 876, 862, 91, 4, 16, 32, 39, 87, 1745, 506, 1745, 888, 5, 87, 528, 4, 145, 124, 93, 101, 150, 3466, 231, 164, 133, 174, 39, 101, 565, 130, 326, 524, 586, 108, 11, 18010, 9479, 42, 39979, 4, 48, 596, 581, 50, 155, 707, 1358, 1443, 345, 1455, 1411, 1123, 455, 413, 323, 12048, 483, 366, 4850, 14, 6215, 9488, 653, 266, 82, 337, 1023, 371, 521, 263, 204, 62, 78, 99, 191, 28, 7, 689, 65, 13, 4850, 269, 266, 82, 337, 1023, 77, 12043, 770, 137, 4, 47, 699, 506, 101, 361, 201, 9, 826, 52, 4177, 756, 387, 369, 52, 4, 297, 413, 86, 763, 27, 247, 98, 3887, 444, 48, 29, 247, 98, 629, 163, 868, 25, 506, 101, 361, 4, 79, 87, 326, 378, 290, 377, 101, 565, 4, 596, 581, 50, 8, 65, 314, 73, 5, 1123, 1600, 413, 323, 12043, 153, 187, 58, 512, 5, 1512, 73, 1664, 565, 135, 517, 57, 41, 5, 10, 385, 120, 1512, 73, 369, 52, 4, 48, 22, 9, 344, 813, 912, 101, 12, 5, 754, 2337, 6, 754, 2880, 43, 702, 96, 792, 207, 4, 510, 735, 541, 1101, 1989, 21, 4, 175, 2873, 1600, 101, 207, 263, 1308, 1158, 4, 84, 195, 175, 29, 1512, 73, 101, 2873, 1600, 263, 217, 37, 262, 82, 691, 736, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "121\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(train_ds['input_ids'][idx])\n",
    "    print(train_ds['token_type_ids'][idx])\n",
    "    print(train_ds['start_positions'][idx])\n",
    "    print(train_ds['end_positions'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从以上结果可以看出，数据集中的example已经被转换成了模型可以接收的feature，包括input_ids、token_type_ids、答案的起始位置等信息。\n",
    "其中：\n",
    "\n",
    "* `input_ids`: 表示输入文本的token ID。\n",
    "* `token_type_ids`: 表示对应的token属于输入的问题还是答案。（Transformer类预训练模型支持单句以及句对输入）。\n",
    "* `overflow_to_sample`: feature对应的example的编号。\n",
    "* `offset_mapping`: 每个token的起始字符和结束字符在原文中对应的index（用于生成答案文本）。\n",
    "* `start_positions`: 答案在这个feature中的开始位置。\n",
    "* `end_positions`: 答案在这个feature中的结束位置。\n",
    "\n",
    "数据处理的详细过程请参见`utils.py`。\n",
    "\n",
    "更多有关数据处理的内容，请参考[数据处理](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Batchify和数据读入\n",
    "\n",
    "使用`paddle.io.BatchSampler`和`paddlenlp.data`中提供的方法把数据组成batch。\n",
    "\n",
    "然后使用`paddle.io.DataLoader`接口多线程异步加载数据。\n",
    "\n",
    "`batchify_fn`详解：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30e43d4659384375a2a2c1b890ca5a995c4324d7168e49cebf1d2a1e99161f7d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddlenlp.data import DataCollatorWithPadding\r\n",
    "\r\n",
    "batch_size = 12\r\n",
    "\r\n",
    "# 定义BatchSampler\r\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "        train_ds, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "dev_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    dev_ds, batch_size=batch_size, shuffle=False)\r\n",
    "\r\n",
    "test_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    test_ds, batch_size=batch_size, shuffle=False)\r\n",
    "\r\n",
    "# 定义batchify_fn\r\n",
    "\r\n",
    "train_batchify_fn = DataCollatorWithPadding(tokenizer)\r\n",
    "\r\n",
    "dev_batchify_fn = DataCollatorWithPadding(tokenizer)\r\n",
    "\r\n",
    "# 构造DataLoader\r\n",
    "train_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_sampler=train_batch_sampler,\r\n",
    "    collate_fn=train_batchify_fn,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "dev_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds_for_model,\r\n",
    "    batch_sampler=dev_batch_sampler,\r\n",
    "    collate_fn=dev_batchify_fn,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "test_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=test_ds_for_model,\r\n",
    "    batch_sampler=test_batch_sampler,\r\n",
    "    collate_fn=dev_batchify_fn,\r\n",
    "    return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "更多PaddleNLP内置的batchify相关API，请参考[collate](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.data.collate.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "到这里数据集准备就全部完成了，下一步我们需要组网并设计loss function。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fdcb44a00ede4ce08ae2652931556fb58cc903f686bf491792489353d2800e7d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型结构\n",
    "\n",
    "## 使用PaddleNLP一键加载预训练模型\n",
    "\n",
    "以下项目以ERNIE为例，介绍如何将预训练模型Fine-tune完成DuReader<sub>robust</sub>阅读理解任务。\n",
    "\n",
    "DuReader<sub>robust</sub>阅读理解任务的本质是答案抽取任务。根据输入的问题和文章，从预训练模型的**sequence_output**中预测答案在文章中的起始位置和结束位置。原理如下图所示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/bb1396fc12614dbabcfb4fcfafe9346507d4d65d0a194d75aba04b9d31bace6b)\n",
    "\n",
    "目前PaddleNLP已经内置了包括ERNIE在内的多种基于预训练模型的常用任务的下游网络，包括机器阅读理解。\n",
    "\n",
    "这些网络在`paddlenlp.transformers`下，均可实现一键调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-28 16:37:22,959] [    INFO] - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForQuestionAnswering'> to load 'ernie-3.0-base-zh'.\n",
      "[2022-06-28 16:37:22,960] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\n",
      "W0628 16:37:22.963842 20601 gpu_context.cc:278] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0628 16:37:22.967849 20601 gpu_context.cc:306] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForQuestionAnswering\r\n",
    "\r\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设计loss function\n",
    "\n",
    "模型的网络结构确定后我们就可以设计loss function了。\n",
    "\n",
    "AutoModelForQuestionAnswering模型对将ErnieModel的sequence_output拆开成start_logits和end_logits输出，所以DuReader<sub>robust</sub>的loss由start_loss和end_loss两部分组成，我们需要自己定义loss function。\n",
    "\n",
    "对于答案起始位置和结束位置的预测可以分别看成两个分类任务。所以设计的loss function如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLossForRobust(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(CrossEntropyLossForRobust, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, y, label):\r\n",
    "        start_logits, end_logits = y\r\n",
    "        start_position, end_position = label\r\n",
    "        start_position = paddle.unsqueeze(start_position, axis=-1)\r\n",
    "        end_position = paddle.unsqueeze(end_position, axis=-1)\r\n",
    "        start_loss = paddle.nn.functional.cross_entropy(\r\n",
    "            input=start_logits, label=start_position)\r\n",
    "        end_loss = paddle.nn.functional.cross_entropy(\r\n",
    "            input=end_logits, label=end_position)\r\n",
    "        loss = (start_loss + end_loss) / 2\r\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "选择网络结构后，我们需要设置Fine-Tune优化策略。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7eca6595f338409498149cb586c077ba4933739810cf436080a2292be7e0a92d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略\n",
    "适用于ERNIE/BERT这类Transformer模型的学习率为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p>\n",
    "<br><center>图3：动态学习率示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练过程中的最大学习率\r\n",
    "learning_rate = 3e-5 \r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 2\r\n",
    "\r\n",
    "# 学习率预热比例\r\n",
    "warmup_proportion = 0.1\r\n",
    "\r\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# 学习率衰减策略\r\n",
    "lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\r\n",
    "\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "现在万事俱备，我们可以开始训练阅读理解模型啦。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6975542d488f4f75b385fe75d574a3aaa8e208f5e99f4acd8a8e8aea3b85c058)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data。\n",
    "2. 将batch data喂给model，做前向计算。\n",
    "3. 将前向计算结果传给损失函数，计算loss。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch时，程序通过evaluate()调用paddlenlp.metric.squad中的squad_evaluate(), compute_predictions()评估当前模型训练的效果，其中：\n",
    "\n",
    "* compute_predictions()用于生成可提交的答案；\n",
    "\n",
    "* squad_evaluate()用于返回评价指标。\n",
    "\n",
    "二者适用于所有符合squad数据格式的答案抽取任务。这类任务使用F1和exact来评估预测的答案和真实答案的相似程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100, epoch: 1, batch: 100, loss: 4.31947\n",
      "global step 200, epoch: 1, batch: 200, loss: 1.50273\n",
      "global step 300, epoch: 1, batch: 300, loss: 1.38390\n",
      "global step 400, epoch: 1, batch: 400, loss: 1.27645\n",
      "global step 500, epoch: 1, batch: 500, loss: 1.43703\n",
      "global step 600, epoch: 1, batch: 600, loss: 1.30268\n",
      "global step 700, epoch: 1, batch: 700, loss: 1.06415\n",
      "global step 800, epoch: 1, batch: 800, loss: 2.09669\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.87356\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 1.44776\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.77803\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 1.50059\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 1.02779\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 1.14965\n",
      "global step 1500, epoch: 2, batch: 29, loss: 1.23338\n",
      "global step 1600, epoch: 2, batch: 129, loss: 1.45627\n",
      "global step 1700, epoch: 2, batch: 229, loss: 1.28679\n",
      "global step 1800, epoch: 2, batch: 329, loss: 0.83966\n",
      "global step 1900, epoch: 2, batch: 429, loss: 1.11859\n",
      "global step 2000, epoch: 2, batch: 529, loss: 0.92468\n",
      "global step 2100, epoch: 2, batch: 629, loss: 0.58391\n",
      "global step 2200, epoch: 2, batch: 729, loss: 0.90198\n",
      "global step 2300, epoch: 2, batch: 829, loss: 1.04647\n",
      "global step 2400, epoch: 2, batch: 929, loss: 1.01556\n",
      "global step 2500, epoch: 2, batch: 1029, loss: 0.93802\n",
      "global step 2600, epoch: 2, batch: 1129, loss: 0.69602\n",
      "global step 2700, epoch: 2, batch: 1229, loss: 1.43215\n",
      "global step 2800, epoch: 2, batch: 1329, loss: 0.87333\n",
      "global step 2900, epoch: 2, batch: 1429, loss: 0.64515\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate\r\n",
    "\r\n",
    "criterion = CrossEntropyLossForRobust()\r\n",
    "global_step = 0\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        global_step += 1\r\n",
    "        input_ids, segment_ids, start_positions, end_positions = batch\r\n",
    "        logits = model(input_ids=batch[\"input_ids\"], token_type_ids=batch[\"token_type_ids\"])\r\n",
    "        loss = criterion(logits, (batch[\"start_positions\"], batch[\"end_positions\"]))\r\n",
    "\r\n",
    "        if global_step % 100 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f\" % (global_step, epoch, step, loss))\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example: 1000\n",
      "time per 1000: 10.584674596786499\n",
      "Processing example: 2000\n",
      "time per 1000: 10.188016891479492\n",
      "Processing example: 3000\n",
      "time per 1000: 9.92471432685852\n",
      "Processing example: 4000\n",
      "time per 1000: 10.358511924743652\n",
      "Processing example: 5000\n",
      "time per 1000: 10.093605041503906\n",
      "Processing example: 6000\n",
      "time per 1000: 10.080277681350708\n",
      "Processing example: 7000\n",
      "time per 1000: 9.987485647201538\n",
      "Processing example: 8000\n",
      "time per 1000: 10.222150325775146\n",
      "Processing example: 9000\n",
      "time per 1000: 10.706721782684326\n",
      "Processing example: 10000\n",
      "time per 1000: 10.512943506240845\n",
      "Processing example: 11000\n",
      "time per 1000: 10.303147554397583\n",
      "Processing example: 12000\n",
      "time per 1000: 10.55568528175354\n",
      "Processing example: 13000\n",
      "time per 1000: 10.26531982421875\n",
      "Processing example: 14000\n",
      "time per 1000: 10.266725778579712\n",
      "Processing example: 15000\n",
      "time per 1000: 10.64447546005249\n",
      "Processing example: 16000\n",
      "time per 1000: 9.920329093933105\n",
      "Processing example: 17000\n",
      "time per 1000: 10.208895206451416\n",
      "Processing example: 18000\n",
      "time per 1000: 10.26574993133545\n",
      "Processing example: 19000\n",
      "time per 1000: 9.914969682693481\n",
      "Processing example: 20000\n",
      "time per 1000: 10.20920467376709\n",
      "Processing example: 21000\n",
      "time per 1000: 10.57719898223877\n",
      "Processing example: 22000\n",
      "time per 1000: 10.71744441986084\n",
      "Processing example: 23000\n",
      "time per 1000: 10.45024824142456\n",
      "Processing example: 24000\n",
      "time per 1000: 10.221151113510132\n",
      "Processing example: 25000\n",
      "time per 1000: 10.319378137588501\n",
      "Processing example: 26000\n",
      "time per 1000: 10.167399168014526\n",
      "Processing example: 27000\n",
      "time per 1000: 10.86082124710083\n",
      "Processing example: 28000\n",
      "time per 1000: 10.394505500793457\n",
      "Processing example: 29000\n",
      "time per 1000: 10.35581088066101\n",
      "Processing example: 30000\n",
      "time per 1000: 10.169897556304932\n",
      "Processing example: 31000\n",
      "time per 1000: 9.831788301467896\n",
      "Processing example: 32000\n",
      "time per 1000: 10.149449348449707\n",
      "Processing example: 33000\n",
      "time per 1000: 10.106878757476807\n",
      "Processing example: 34000\n",
      "time per 1000: 10.475741624832153\n",
      "Processing example: 35000\n",
      "time per 1000: 9.970959901809692\n",
      "Processing example: 36000\n",
      "time per 1000: 10.39727783203125\n",
      "Processing example: 37000\n",
      "time per 1000: 9.979295015335083\n",
      "Processing example: 38000\n",
      "time per 1000: 10.217779397964478\n",
      "Processing example: 39000\n",
      "time per 1000: 10.340791702270508\n",
      "Processing example: 40000\n",
      "time per 1000: 10.340667247772217\n",
      "Processing example: 41000\n",
      "time per 1000: 10.436401605606079\n",
      "Processing example: 42000\n",
      "time per 1000: 10.371085405349731\n",
      "Processing example: 43000\n",
      "time per 1000: 10.644593238830566\n",
      "Processing example: 44000\n",
      "time per 1000: 10.25355052947998\n",
      "Processing example: 45000\n",
      "time per 1000: 10.114908933639526\n",
      "Processing example: 46000\n",
      "time per 1000: 10.181347131729126\n",
      "Processing example: 47000\n",
      "time per 1000: 10.038010835647583\n",
      "Processing example: 48000\n",
      "time per 1000: 10.707454204559326\n",
      "Processing example: 49000\n",
      "time per 1000: 10.093716859817505\n",
      "Processing example: 50000\n",
      "time per 1000: 9.750082731246948\n",
      "Processing example: 51000\n",
      "time per 1000: 10.104126214981079\n",
      "Processing example: 52000\n",
      "time per 1000: 10.051070213317871\n",
      "Processing example: 53000\n",
      "time per 1000: 9.974929094314575\n",
      "Processing example: 54000\n",
      "time per 1000: 10.26790714263916\n",
      "Processing example: 55000\n",
      "time per 1000: 10.284163236618042\n",
      "Processing example: 56000\n",
      "time per 1000: 10.124592542648315\n",
      "Processing example: 57000\n",
      "time per 1000: 10.628040075302124\n",
      "Processing example: 58000\n",
      "time per 1000: 10.414206981658936\n",
      "Processing example: 59000\n",
      "time per 1000: 10.146691083908081\n",
      "Processing example: 60000\n",
      "time per 1000: 10.072739124298096\n",
      "Processing example: 61000\n",
      "time per 1000: 9.900139808654785\n",
      "Processing example: 62000\n",
      "time per 1000: 10.332890510559082\n",
      "Processing example: 63000\n",
      "time per 1000: 10.174994707107544\n",
      "Processing example: 64000\n",
      "time per 1000: 10.424129486083984\n",
      "Processing example: 65000\n",
      "time per 1000: 10.393558740615845\n",
      "Processing example: 66000\n",
      "time per 1000: 10.283146142959595\n",
      "Processing example: 67000\n",
      "time per 1000: 9.896865606307983\n",
      "Processing example: 68000\n",
      "time per 1000: 9.99543833732605\n",
      "Processing example: 69000\n",
      "time per 1000: 10.295023441314697\n",
      "\n",
      "问题： 220v一安等于多少瓦\n",
      "原文： 在220交流电的状态下一安等于220瓦.基于32太1.5匹用多大的开关计算方法是：1匹=0.735瓦. 0.735*1.5*32=35.28千瓦  1千瓦=4.5安  35.28*4.5=158.26安  这儿是实际的电流,在现实应用过程中不能用160安的  开关,单个1.5匹启动时有一个较大的启动电流,在实际用用是乘以105倍：158.26*1.5=237.39安. 开关的电流应该是250A的空气开关.\n",
      "答案： 220瓦\n",
      "\n",
      "问题： 氧化铜和稀盐酸的离子方程式\n",
      "原文： 化学方程式：CuO+2HCl=CuCl2+H2O 书写离子方程式时,只有强电解质（强酸、强碱、盐）拆开写成离子形式. 离子方程式： CuO+2H+=Cu^2+ +H2O\n",
      "答案： CuO+2H+=Cu^2+ +H2O\n",
      "\n",
      "问题： 刀塔传奇98元英雄排行\n",
      "原文： 让我们把目光放到刀塔传奇中去,看看那些在竞技场的刀山火海中异军突起的英雄,很多只是得益于一个小小的改动,却改变了竞技场的整个格局。http://www.18183.com/dtcq/syzs/wanjiayc/164981.html|1艾吉奥梦境打架都可以2科学怪人梦境团本高分必备3魔像可以打吸血鬼一个梦境4白银很看好她期待她的觉醒5凹凸曼打猴子,其他可有可无\n",
      "答案： 艾吉奥\n",
      "\n",
      "问题： 新款汉兰达什么时候上市\n",
      "原文： 新一代丰田汉兰达虽然早在2013年的纽约车展上已经正式亮相，但依然迟迟未见在国内出现。据目前获得的最新消息称，现款汉兰达将于2015年1月正式停产，新一代车型预计将于2015年一季度末投产，并于2015年上半年正式亮相。目前广汽丰田新一代汉兰达申报图已经在网上曝光。\n",
      "答案： 2015年上半年\n",
      "\n",
      "问题： 一立方分米等于多少立方米\n",
      "原文： 正方体用米作单位计算：  1*1*1=1立方米 把米化成分米做单位就是10分米,用分米作单位计算：  10*10*10=1000立方分米 所以,1立方米=1000立方分米\n",
      "答案： 1000立方分米\n",
      "\n",
      "问题： 橙子和什么一起榨汁好喝\n",
      "原文： 醇厚：香蕉，木瓜，火龙果，草莓，芒果，杏子。 清新：柠檬，黄瓜，雪梨，西瓜，柚子，芹菜，哈密瓜难以区别：西红柿，苹果，橙子，桃子，猕猴桃，菠萝，柚子，樱桃 很多人都试过：苹果+橙子+雪梨；雪梨+黄瓜；柚子+苹果；西红柿+柠檬+柚子\n",
      "答案： 雪梨\n"
     ]
    }
   ],
   "source": [
    "# 传入test_data_loader，并将is_test参数设为True，即可生成千言比赛可提交的结果。\r\n",
    "evaluate(model=model, raw_dataset=test_examples, dataset=test_ds, data_loader=test_data_loader, is_test=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上基线实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5970708b1c584e73bff9f8937685f5477f7fed902a164cd184bffb073307ce61)\n",
    "\n",
    "\n",
    "**更多使用方法可参考PaddleNLP教程**\n",
    "\n",
    "- [使用seq2vec模块进行句子情感分类](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "- [使用预训练模型ERNIE优化情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    "- [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    "- [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "- [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    "- [使用预训练模型ERNIE-GEN自动写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    "- [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    "- [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img width=\"300\" alt=\"image\" src=\"https://ai-studio-static-online.cdn.bcebos.com/86ac11bf3fb5491894317ed278fb4e24cb78600132e64faa997a7a39004b7775\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
